{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6745b857",
   "metadata": {},
   "source": [
    "# Module3 Hand on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb65263",
   "metadata": {},
   "source": [
    "## prepraring python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb155975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # do some data\n",
    "import string\n",
    "import timeit # just import for timer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e99547",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360f4d0",
   "metadata": {},
   "source": [
    "## previous code from previous module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9d5f6",
   "metadata": {},
   "source": [
    "### Retrive Data from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a01730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_data():\n",
    "    data = pd.read_csv('./data/software_developer_united_states_1971_20191023_1.csv')\n",
    "    description = data['job_description']\n",
    "    cleaned_description = description.apply(lambda s: s.translate(str.maketrans('', '', string.punctuation + u'\\xa0')))\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.lower())\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.translate(str.maketrans(string.whitespace, ' '*len(string.whitespace), '')))\n",
    "    cleaned_description = cleaned_description.drop_duplicates()\n",
    "    return cleaned_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c243d06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683055d8",
   "metadata": {},
   "source": [
    "# Hand on #1 the bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd06bee",
   "metadata": {},
   "source": [
    "## A suggested Scikit-learn approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1024e9d",
   "metadata": {},
   "source": [
    "### Setting up the preProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46b0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(s):\n",
    "    ps = PorterStemmer()\n",
    "    s = word_tokenize(s)\n",
    "    stopwords_set = set(stopwords.words())\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = [ps.stem(w) for w in s]\n",
    "    s = ' '.join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324b41d",
   "metadata": {},
   "source": [
    "### Do some query ( with java before python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9d2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_vectorize():\n",
    "    cleaned_description = get_and_clean_data()\n",
    "    vectorizer = CountVectorizer(preprocessor=preProcess)\n",
    "    vectorizer.fit_transform(cleaned_description)\n",
    "    query = vectorizer.transform(['good at java and python'])\n",
    "    print(query)\n",
    "    print(vectorizer.inverse_transform(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977be45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15571)\t1\n",
      "  (0, 18608)\t1\n",
      "  (0, 26294)\t1\n",
      "[array(['good', 'java', 'python'], dtype='<U182')]\n"
     ]
    }
   ],
   "source": [
    "sk_vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a67601",
   "metadata": {},
   "source": [
    "### Do another query ( with python before java)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1d1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_vectorize2():\n",
    "    cleaned_description = get_and_clean_data()\n",
    "    vectorizer = CountVectorizer(preprocessor=preProcess)\n",
    "    vectorizer.fit_transform(cleaned_description)\n",
    "    query = vectorizer.transform(['good at python and java'])\n",
    "    print(query)\n",
    "    print(vectorizer.inverse_transform(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed65a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15571)\t1\n",
      "  (0, 18608)\t1\n",
      "  (0, 26294)\t1\n",
      "[array(['good', 'java', 'python'], dtype='<U182')]\n"
     ]
    }
   ],
   "source": [
    "sk_vectorize2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c40fc7",
   "metadata": {},
   "source": [
    "#### you can see that the result from `sk_vectorize()` and `sk_vectorize2()` are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf22e0",
   "metadata": {},
   "source": [
    "### Handing sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf6d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_vectorize3():\n",
    "    cleaned_description = get_and_clean_data()\n",
    "    vectorizer = CountVectorizer(preprocessor=preProcess,ngram_range=(1,2))\n",
    "    vectorizer.fit_transform(cleaned_description)\n",
    "    query = vectorizer.transform(['good at python and java'])\n",
    "    print(query)\n",
    "    print(vectorizer.inverse_transform(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60fa97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 168556)\t1\n",
      "  (0, 203737)\t1\n",
      "  (0, 302056)\t1\n",
      "  (0, 302204)\t1\n",
      "[array(['good', 'java', 'python', 'python java'], dtype='<U274')]\n"
     ]
    }
   ],
   "source": [
    "sk_vectorize3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0e89e",
   "metadata": {},
   "source": [
    "### The feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af1affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_vectorize4():\n",
    "    cleaned_description = get_and_clean_data()\n",
    "    vectorizer = CountVectorizer(preprocessor=preProcess,ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(cleaned_description)\n",
    "    print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e4836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sk_vectorize4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7ea10",
   "metadata": {},
   "source": [
    "#### unforturnary my computer is not fast enough, T-T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d3b3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9fe69",
   "metadata": {},
   "source": [
    "# Hand on #2 tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bec3c",
   "metadata": {},
   "source": [
    "### tf-idf weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "467bc4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform:\n",
      "[[0 1 1 ... 5 1 0]\n",
      " [1 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 1 0 1]]\n",
      "=================\n",
      "\n",
      "After transform:\n",
      "[[0.         0.36736504 0.36736504 ... 0.33466756 0.36736504 0.        ]\n",
      " [0.27674598 0.         0.         ... 0.12946708 0.         0.27674598]\n",
      " [0.         0.         0.         ... 0.20520047 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.27674598 0.         0.         ... 0.12946708 0.         0.27674598]]\n",
      "=================\n",
      "\n",
      "The summary :\n",
      "     110000     18000      1983       250    300000        34       510  \\\n",
      "0  0.000000  0.367365  0.367365  0.367365  0.367365  0.000000  0.000000   \n",
      "1  0.276746  0.000000  0.000000  0.000000  0.000000  0.000000  0.276746   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.367365  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.276746  0.000000  0.000000  0.000000  0.000000  0.000000  0.276746   \n",
      "\n",
      "      62304      8000  8882376835  ...    within   without      work  \\\n",
      "0  0.367365  0.367365    0.367365  ...  0.367365  0.276746  0.214174   \n",
      "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.107087   \n",
      "2  0.000000  0.000000    0.000000  ...  0.000000  0.276746  0.248648   \n",
      "3  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.107087   \n",
      "4  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.107087   \n",
      "\n",
      "      would     write    writer       xml      year     yield      zaur  \n",
      "0  0.276746  0.317703  0.367365  0.000000  0.334668  0.367365  0.000000  \n",
      "1  0.000000  0.200449  0.000000  0.276746  0.129467  0.000000  0.276746  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.205200  0.000000  0.000000  \n",
      "3  0.276746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.200449  0.000000  0.276746  0.129467  0.000000  0.276746  \n",
      "\n",
      "[5 rows x 414 columns]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cleaned_des = get_and_clean_data()\n",
    "cleaned_des = cleaned_des.iloc[:N]\n",
    "vectez = CountVectorizer(preprocessor=preProcess)\n",
    "X = vectez.fit_transform(cleaned_des)\n",
    "print(\"Before transform:\")\n",
    "print(X.toarray())\n",
    "print(\"=================\")\n",
    "print()\n",
    "\n",
    "print(\"After transform:\")\n",
    "X.data = np.log10(X.data + 1)\n",
    "X = X.multiply(np.log10(N / X.sum(0))[0])\n",
    "\n",
    "print(X.toarray())\n",
    "print(\"=================\")\n",
    "print()\n",
    "\n",
    "print(\"The summary :\")\n",
    "print(pd.DataFrame(X.toarray(),columns=vectez.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61146f8",
   "metadata": {},
   "source": [
    "### Scikit-learn builtin tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07337956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     110000     18000      1983       250    300000        34       510  \\\n",
      "0  0.000000  0.045564  0.045564  0.045564  0.045564  0.000000  0.000000   \n",
      "1  0.074481  0.000000  0.000000  0.000000  0.000000  0.000000  0.074481   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.081564  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.074785  0.000000  0.000000  0.000000  0.000000  0.000000  0.074785   \n",
      "\n",
      "      62304      8000  8882376835  ...    within   without      work  \\\n",
      "0  0.045564  0.045564    0.045564  ...  0.045564  0.036761  0.065135   \n",
      "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.043990   \n",
      "2  0.000000  0.000000    0.000000  ...  0.000000  0.065805  0.155462   \n",
      "3  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.043996   \n",
      "4  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.044169   \n",
      "\n",
      "      would     write    writer       xml      year     yield      zaur  \n",
      "0  0.036761  0.061030  0.045564  0.000000  0.128351  0.045564  0.000000  \n",
      "1  0.000000  0.061826  0.000000  0.074481  0.052010  0.000000  0.074481  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.091903  0.000000  0.000000  \n",
      "3  0.074491  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.062078  0.000000  0.074785  0.052222  0.000000  0.074785  \n",
      "\n",
      "[5 rows x 414 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "N = 5\n",
    "cleaned_des2 = get_and_clean_data()\n",
    "cleaned_des2 = cleaned_des2.iloc[:N]\n",
    "vectz = TfidfVectorizer(preprocessor=preProcess)\n",
    "X = vectz.fit_transform(cleaned_des2)\n",
    "print(pd.DataFrame(X.toarray(),columns=vectz.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8cfd01",
   "metadata": {},
   "source": [
    "#### What a nice tf-idf built in from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e34b23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318fd4a",
   "metadata": {},
   "source": [
    "# Hand on #3 BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcb29b",
   "metadata": {},
   "source": [
    "## The base code from [`This github gist`](https://gist.github.com/koreyou/f3a8a0470d32aa56b32f198f49a9f2b8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "888dac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25(object):\n",
    "    def __init__(self, b=0.75, k1=1.6):\n",
    "        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f009ee",
   "metadata": {},
   "source": [
    "### BM25 and Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0a795f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         1.89200257 2.11154135 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "N = 5\n",
    "cleaned_des3 = get_and_clean_data()\n",
    "cleaned_des3 = cleaned_des3.iloc[:N]\n",
    "bm25 = BM25()\n",
    "bm25.fit(cleaned_des3)\n",
    "print(bm25.transform('aws github',cleaned_des3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c0d13",
   "metadata": {},
   "source": [
    "#### ended hand on 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
